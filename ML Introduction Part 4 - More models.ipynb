{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Naive Bayes over Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "def load_dfs(test_size=0.1, shuffle=False, verbose=True):\n",
    "    df = pd.read_csv(\"data/train.csv\")\n",
    "    df = df.drop(['keyword', 'location'], axis=1)\n",
    "    \n",
    "    df_test = pd.read_csv(\"data/test.csv\")\n",
    "    df_test = df_test.drop(['keyword', 'location'], axis=1)\n",
    "    \n",
    "    df_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "    \n",
    "    df_train, df_val = train_test_split(df, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"train shape: {df_train.shape}\")\n",
    "        print(f\"val shape  : {df_val.shape}\")\n",
    "        print(f\"test shape : {df_test.shape}\")\n",
    "    return df_train, df_val, df_test, df_sub\n",
    "\n",
    "\n",
    "def count_vect_pipeline(df_train, df_val, df_test, \n",
    "                        vectorizer, model_fn,\n",
    "                        create_submission=False):\n",
    "    X_train = vectorizer.fit_transform(df_train['text'])\n",
    "    df_feature_train = pd.DataFrame(X_train.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    X_val = vectorizer.transform(df_val['text'])\n",
    "    df_feature_val = pd.DataFrame(X_val.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    X_test = vectorizer.transform(df_test['text'])\n",
    "    df_feature_test = pd.DataFrame(X_test.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    \n",
    "    y_pred_train = model_fn(df_feature_train)\n",
    "    y_pred_val = model_fn(df_feature_val)\n",
    "    y_pred_test = model_fn(df_feature_test)\n",
    "    \n",
    "    train_acc = accuracy_score(df_train['target'], y_pred_train)\n",
    "    val_acc = accuracy_score(df_val['target'], y_pred_val)\n",
    "    print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Val accuracy  : {val_acc:.3f}\")\n",
    "    \n",
    "    df_test['target'] = y_pred_test\n",
    "    if create_submission is not False:\n",
    "        df_test[['id', 'target']].to_csv(f\"{create_submission}.csv\", index=False)\n",
    "        \n",
    "    return df_test[['id', 'target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (6851, 3)\n",
      "val shape  : (762, 3)\n",
      "test shape : (3263, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test, df_sub = load_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(df, drop_stopwords=True, keep_only_alpha=True):\n",
    "    l = flatten(df['text'].str.lower().str.split().tolist())\n",
    "    if drop_stopwords:\n",
    "        st = stopwords.words('english')\n",
    "        l = [w for w in l if w not in st and (not keep_only_alpha or w.isalpha())]\n",
    "    return Counter(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fire', 138),\n",
       " ('via', 110),\n",
       " ('suicide', 100),\n",
       " ('people', 87),\n",
       " ('like', 80),\n",
       " ('killed', 75),\n",
       " ('police', 70),\n",
       " ('california', 69),\n",
       " ('two', 67),\n",
       " ('disaster', 67),\n",
       " ('train', 66),\n",
       " ('emergency', 65),\n",
       " ('crash', 65),\n",
       " ('bombing', 63),\n",
       " ('bomb', 63),\n",
       " ('get', 62),\n",
       " ('families', 61),\n",
       " ('buildings', 61),\n",
       " ('burning', 60),\n",
       " ('news', 58),\n",
       " ('bomber', 57),\n",
       " ('atomic', 56),\n",
       " ('hiroshima', 56),\n",
       " ('still', 53),\n",
       " ('fatal', 53),\n",
       " ('one', 52),\n",
       " ('nuclear', 52),\n",
       " ('accident', 49),\n",
       " ('new', 49),\n",
       " ('years', 48),\n",
       " ('debris', 48),\n",
       " ('storm', 48),\n",
       " ('homes', 48),\n",
       " ('may', 47),\n",
       " ('watch', 47),\n",
       " ('attack', 46),\n",
       " ('northern', 46),\n",
       " ('collapse', 46),\n",
       " ('mass', 46),\n",
       " ('first', 45),\n",
       " ('forest', 44),\n",
       " ('near', 44),\n",
       " ('car', 44),\n",
       " ('dead', 44),\n",
       " ('war', 44),\n",
       " ('severe', 43),\n",
       " ('oil', 43),\n",
       " ('fires', 42),\n",
       " ('man', 40),\n",
       " ('army', 40)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_disaster = get_vocab(df_train[df_train['target'] == 1])\n",
    "V_disaster.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB: 65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.684\n",
      "Val accuracy  : 0.707\n"
     ]
    }
   ],
   "source": [
    "# LB score: 0.65124\n",
    "disaster_words = ['pain', 'trauma', 'tornado', 'crash', \n",
    "                  'hurricane', 'flood', 'dead', 'death', 'fire', 'forest',\n",
    "                  'suicide', 'killed', 'police', 'disaster', 'train', 'emergency', 'bombing', 'bomb',\n",
    "                  'families', 'burning', 'news', 'bomber', 'atomic', 'hiroshima', 'fatal', 'nuclear',\n",
    "                  'accident', 'storm', 'homes', 'attack', 'car', 'war', 'severe', 'oil', 'fires', 'army']\n",
    "\n",
    "def model(sample):\n",
    "    for feature in disaster_words:\n",
    "        if sample[feature] != 0:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def handcrafted_model(df):\n",
    "    y_pred = df.apply(model, axis=1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "vect = CountVectorizer(vocabulary=disaster_words)\n",
    "df_sub1 = count_vect_pipeline(df_train, df_val, df_test, vect, handcrafted_model, \n",
    "                              create_submission='handcrafted-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 211),\n",
       " ('get', 149),\n",
       " ('new', 143),\n",
       " ('body', 105),\n",
       " ('one', 103),\n",
       " ('via', 88),\n",
       " ('would', 86),\n",
       " ('full', 81),\n",
       " ('emergency', 75),\n",
       " ('got', 74),\n",
       " ('love', 72),\n",
       " ('people', 72),\n",
       " ('know', 70),\n",
       " ('see', 70),\n",
       " ('video', 69),\n",
       " ('going', 67),\n",
       " ('time', 64),\n",
       " ('back', 64),\n",
       " ('want', 61),\n",
       " ('go', 56),\n",
       " ('think', 56),\n",
       " ('fire', 56),\n",
       " ('first', 54),\n",
       " ('still', 54),\n",
       " ('day', 54),\n",
       " ('u', 54),\n",
       " ('last', 53),\n",
       " ('make', 51),\n",
       " ('burning', 51),\n",
       " ('us', 50),\n",
       " ('need', 50),\n",
       " ('really', 49),\n",
       " ('let', 49),\n",
       " ('man', 48),\n",
       " ('good', 48),\n",
       " ('many', 47),\n",
       " ('even', 45),\n",
       " ('take', 45),\n",
       " ('world', 44),\n",
       " ('lol', 44),\n",
       " ('way', 43),\n",
       " ('feel', 43),\n",
       " ('fear', 43),\n",
       " ('say', 42),\n",
       " ('work', 42),\n",
       " ('cross', 42),\n",
       " ('every', 40),\n",
       " ('never', 40),\n",
       " ('life', 39),\n",
       " ('read', 39),\n",
       " ('help', 39),\n",
       " ('im', 38),\n",
       " ('content', 37),\n",
       " ('much', 36),\n",
       " ('check', 36),\n",
       " ('top', 36),\n",
       " ('could', 36),\n",
       " ('may', 36),\n",
       " ('ruin', 36),\n",
       " ('screaming', 35),\n",
       " ('bloody', 35),\n",
       " ('great', 35),\n",
       " ('right', 35),\n",
       " ('bag', 35),\n",
       " ('look', 34),\n",
       " ('another', 34),\n",
       " ('face', 34),\n",
       " ('god', 34),\n",
       " ('fucking', 34),\n",
       " ('ever', 33),\n",
       " ('without', 33),\n",
       " ('smoke', 33),\n",
       " ('reddit', 33),\n",
       " ('rt', 32),\n",
       " ('stop', 32),\n",
       " ('liked', 32),\n",
       " ('always', 31),\n",
       " ('night', 31),\n",
       " ('police', 31),\n",
       " ('coming', 31),\n",
       " ('getting', 31),\n",
       " ('best', 31),\n",
       " ('bad', 31),\n",
       " ('screamed', 31),\n",
       " ('better', 30),\n",
       " ('school', 30),\n",
       " ('please', 30),\n",
       " ('gonna', 30),\n",
       " ('black', 30),\n",
       " ('fall', 30),\n",
       " ('blew', 30),\n",
       " ('harm', 30),\n",
       " ('loud', 30),\n",
       " ('car', 29),\n",
       " ('head', 29),\n",
       " ('home', 29),\n",
       " ('army', 29),\n",
       " ('hope', 29),\n",
       " ('fuck', 29),\n",
       " ('little', 29),\n",
       " ('put', 29),\n",
       " ('bags', 29),\n",
       " ('shit', 28),\n",
       " ('someone', 28),\n",
       " ('ebay', 28),\n",
       " ('come', 28),\n",
       " ('death', 28),\n",
       " ('shoulder', 28),\n",
       " ('buildings', 28),\n",
       " ('curfew', 28),\n",
       " ('panicking', 28),\n",
       " ('sinking', 28),\n",
       " ('sunk', 28),\n",
       " ('year', 27),\n",
       " ('everyone', 27),\n",
       " ('destroy', 27),\n",
       " ('said', 27),\n",
       " ('drown', 27),\n",
       " ('blown', 27),\n",
       " ('lava', 27),\n",
       " ('quarantine', 27),\n",
       " ('long', 26),\n",
       " ('military', 26),\n",
       " ('burned', 26),\n",
       " ('bleeding', 26),\n",
       " ('ass', 26),\n",
       " ('nuclear', 26),\n",
       " ('trapped', 26),\n",
       " ('crush', 26),\n",
       " ('demolish', 26),\n",
       " ('panic', 26),\n",
       " ('electrocute', 26),\n",
       " ('inundated', 26),\n",
       " ('obliterated', 26),\n",
       " ('next', 25),\n",
       " ('free', 25),\n",
       " ('blood', 25),\n",
       " ('also', 25),\n",
       " ('hot', 25),\n",
       " ('fan', 25),\n",
       " ('girl', 25),\n",
       " ('real', 25),\n",
       " ('made', 25),\n",
       " ('destroyed', 25),\n",
       " ('wanna', 25),\n",
       " ('blazing', 25),\n",
       " ('keep', 25),\n",
       " ('bagging', 25),\n",
       " ('bomb', 25),\n",
       " ('destruction', 25),\n",
       " ('crushed', 25),\n",
       " ('desolation', 25),\n",
       " ('disaster', 25),\n",
       " ('survive', 25),\n",
       " ('drowning', 25),\n",
       " ('explode', 25),\n",
       " ('well', 24),\n",
       " ('attack', 24),\n",
       " ('watch', 24),\n",
       " ('two', 24),\n",
       " ('battle', 24),\n",
       " ('show', 24),\n",
       " ('flattened', 24),\n",
       " ('obliterate', 24),\n",
       " ('set', 23),\n",
       " ('end', 23),\n",
       " ('meltdown', 23),\n",
       " ('phone', 23),\n",
       " ('looks', 23),\n",
       " ('injury', 23),\n",
       " ('heart', 23),\n",
       " ('music', 23),\n",
       " ('big', 23),\n",
       " ('today', 23),\n",
       " ('injuries', 23),\n",
       " ('survived', 23),\n",
       " ('riot', 23),\n",
       " ('summer', 22),\n",
       " ('dead', 22),\n",
       " ('heard', 22),\n",
       " ('play', 22),\n",
       " ('services', 22),\n",
       " ('high', 22),\n",
       " ('things', 22),\n",
       " ('game', 22),\n",
       " ('sure', 22),\n",
       " ('mass', 22),\n",
       " ('oh', 22),\n",
       " ('policy', 22),\n",
       " ('market', 22),\n",
       " ('goes', 22),\n",
       " ('deluge', 22),\n",
       " ('live', 21),\n",
       " ('thank', 21),\n",
       " ('movie', 21),\n",
       " ('years', 21),\n",
       " ('start', 21),\n",
       " ('collapse', 21),\n",
       " ('light', 21),\n",
       " ('send', 21),\n",
       " ('whole', 21),\n",
       " ('baby', 21),\n",
       " ('hazard', 21),\n",
       " ('collide', 21),\n",
       " ('danger', 21),\n",
       " ('storm', 21),\n",
       " ('bang', 21),\n",
       " ('quarantined', 21),\n",
       " ('thunder', 21),\n",
       " ('came', 20),\n",
       " ('already', 20),\n",
       " ('apocalypse', 20),\n",
       " ('armageddon', 20),\n",
       " ('pick', 20),\n",
       " ('sound', 20),\n",
       " ('makes', 20),\n",
       " ('twitter', 20),\n",
       " ('white', 20),\n",
       " ('flames', 20),\n",
       " ('finally', 20),\n",
       " ('horrible', 20),\n",
       " ('ur', 20),\n",
       " ('song', 20),\n",
       " ('woman', 20),\n",
       " ('cliff', 20),\n",
       " ('detonate', 20),\n",
       " ('around', 19),\n",
       " ('used', 19),\n",
       " ('gets', 19),\n",
       " ('making', 19),\n",
       " ('annihilated', 19),\n",
       " ('bar', 19),\n",
       " ('self', 19),\n",
       " ('save', 19),\n",
       " ('since', 19),\n",
       " ('leather', 19),\n",
       " ('thought', 19),\n",
       " ('crash', 19),\n",
       " ('beautiful', 19),\n",
       " ('change', 19),\n",
       " ('cause', 19),\n",
       " ('probably', 19),\n",
       " ('trying', 19),\n",
       " ('collapsed', 19),\n",
       " ('stock', 19),\n",
       " ('care', 18),\n",
       " ('truck', 18),\n",
       " ('actually', 18),\n",
       " ('brown', 18),\n",
       " ('run', 18),\n",
       " ('part', 18),\n",
       " ('hit', 18),\n",
       " ('nothing', 18),\n",
       " ('blight', 18),\n",
       " ('deluged', 18),\n",
       " ('demolished', 18),\n",
       " ('demolition', 18),\n",
       " ('derail', 18),\n",
       " ('flood', 18),\n",
       " ('hellfire', 18),\n",
       " ('mudslide', 18),\n",
       " ('natural', 18),\n",
       " ('obliteration', 18),\n",
       " ('offensive', 18),\n",
       " ('screams', 18),\n",
       " ('wait', 17),\n",
       " ('thing', 17),\n",
       " ('went', 17),\n",
       " ('soon', 17),\n",
       " ('lot', 17),\n",
       " ('feeling', 17),\n",
       " ('remember', 17),\n",
       " ('longer', 17),\n",
       " ('old', 17),\n",
       " ('away', 17),\n",
       " ('left', 17),\n",
       " ('low', 17),\n",
       " ('family', 17),\n",
       " ('plan', 17),\n",
       " ('catastrophe', 17),\n",
       " ('effect', 17),\n",
       " ('banned', 17),\n",
       " ('engulfed', 17),\n",
       " ('exploded', 17),\n",
       " ('fatality', 17),\n",
       " ('lightning', 17),\n",
       " ('siren', 17),\n",
       " ('cool', 16),\n",
       " ('happy', 16),\n",
       " ('kids', 16),\n",
       " ('call', 16),\n",
       " ('bc', 16),\n",
       " ('week', 16),\n",
       " ('tomorrow', 16),\n",
       " ('believe', 16),\n",
       " ('space', 16),\n",
       " ('hollywood', 16),\n",
       " ('dont', 16),\n",
       " ('news', 16),\n",
       " ('cake', 16),\n",
       " ('house', 16),\n",
       " ('hell', 16),\n",
       " ('stay', 16),\n",
       " ('ladies', 16),\n",
       " ('find', 16),\n",
       " ('meek', 16),\n",
       " ('appears', 16),\n",
       " ('bombed', 16),\n",
       " ('collided', 16),\n",
       " ('drowned', 16),\n",
       " ('electrocuted', 16),\n",
       " ('hijacking', 16),\n",
       " ('blast', 16),\n",
       " ('pandemonium', 16),\n",
       " ('miners', 16),\n",
       " ('city', 15),\n",
       " ('ambulance', 15),\n",
       " ('everything', 15),\n",
       " ('ball', 15),\n",
       " ('fun', 15),\n",
       " ('river', 15),\n",
       " ('hours', 15),\n",
       " ('data', 15),\n",
       " ('damn', 15),\n",
       " ('literally', 15),\n",
       " ('book', 15),\n",
       " ('yeah', 15),\n",
       " ('wind', 15),\n",
       " ('women', 15),\n",
       " ('blizzard', 15),\n",
       " ('friends', 15),\n",
       " ('waves', 15),\n",
       " ('damage', 15),\n",
       " ('person', 15),\n",
       " ('ignition', 15),\n",
       " ('knock', 15),\n",
       " ('eyewitness', 15),\n",
       " ('rescue', 15),\n",
       " ('responders', 15),\n",
       " ('led', 15),\n",
       " ('hat', 15),\n",
       " ('subreddits', 15),\n",
       " ('rescued', 15),\n",
       " ('try', 14),\n",
       " ('ablaze', 14),\n",
       " ('side', 14),\n",
       " ('lord', 14),\n",
       " ('accident', 14),\n",
       " ('tell', 14),\n",
       " ('something', 14),\n",
       " ('almost', 14),\n",
       " ('might', 14),\n",
       " ('sirens', 14),\n",
       " ('hate', 14),\n",
       " ('sorry', 14),\n",
       " ('must', 14),\n",
       " ('food', 14),\n",
       " ('maybe', 14),\n",
       " ('national', 14),\n",
       " ('stand', 14),\n",
       " ('hey', 14),\n",
       " ('name', 14),\n",
       " ('tv', 14),\n",
       " ('war', 14),\n",
       " ('lmao', 14),\n",
       " ('attacked', 14),\n",
       " ('guys', 14),\n",
       " ('saw', 14),\n",
       " ('added', 14),\n",
       " ('computers', 14),\n",
       " ('eyes', 14),\n",
       " ('times', 14),\n",
       " ('money', 14),\n",
       " ('tote', 14),\n",
       " ('drake', 14),\n",
       " ('spot', 14),\n",
       " ('hear', 14),\n",
       " ('hazardous', 14),\n",
       " ('chemical', 14),\n",
       " ('evacuate', 14),\n",
       " ('hijacker', 14),\n",
       " ('breaking', 14),\n",
       " ('stretcher', 14),\n",
       " ('awesome', 13),\n",
       " ('leave', 13),\n",
       " ('upon', 13),\n",
       " ('tonight', 13),\n",
       " ('plans', 13),\n",
       " ('guy', 13),\n",
       " ('done', 13),\n",
       " ('seen', 13),\n",
       " ('comes', 13),\n",
       " ('reason', 13),\n",
       " ('friend', 13),\n",
       " ('anything', 13),\n",
       " ('avalanche', 13),\n",
       " ('deal', 13),\n",
       " ('power', 13),\n",
       " ('eye', 13),\n",
       " ('miss', 13),\n",
       " ('yes', 13),\n",
       " ('tweet', 13),\n",
       " ('words', 13),\n",
       " ('soul', 13),\n",
       " ('use', 13),\n",
       " ('online', 13),\n",
       " ('download', 13),\n",
       " ('likely', 13),\n",
       " ('rise', 13),\n",
       " ('released', 13),\n",
       " ('water', 13),\n",
       " ('dust', 13),\n",
       " ('hail', 13),\n",
       " ('fast', 12),\n",
       " ('bring', 12),\n",
       " ('inside', 12),\n",
       " ('seeing', 12),\n",
       " ('die', 12),\n",
       " ('anyone', 12),\n",
       " ('vs', 12),\n",
       " ('join', 12),\n",
       " ('fight', 12),\n",
       " ('totally', 12),\n",
       " ('red', 12),\n",
       " ('class', 12),\n",
       " ('far', 12),\n",
       " ('blue', 12),\n",
       " ('beach', 12),\n",
       " ('blaze', 12),\n",
       " ('cant', 12),\n",
       " ('win', 12),\n",
       " ('men', 12),\n",
       " ('louis', 12),\n",
       " ('album', 12),\n",
       " ('playlist', 12),\n",
       " ('story', 12),\n",
       " ('fedex', 12),\n",
       " ('trust', 12),\n",
       " ('looking', 12),\n",
       " ('move', 12),\n",
       " ('niggas', 12),\n",
       " ('n', 12),\n",
       " ('turn', 12),\n",
       " ('heat', 12),\n",
       " ('pretty', 12),\n",
       " ('yet', 12),\n",
       " ('level', 12),\n",
       " ('biggest', 12),\n",
       " ('half', 12),\n",
       " ('faux', 12),\n",
       " ('womens', 12),\n",
       " ('listen', 12),\n",
       " ('health', 12),\n",
       " ('failure', 12),\n",
       " ('government', 12),\n",
       " ('crashed', 12),\n",
       " ('floods', 12),\n",
       " ('view', 12),\n",
       " ('cree', 12),\n",
       " ('hijack', 12),\n",
       " ('hurricane', 12),\n",
       " ('reactor', 12),\n",
       " ('structural', 12),\n",
       " ('outside', 11),\n",
       " ('wanted', 11),\n",
       " ('hard', 11),\n",
       " ('secret', 11),\n",
       " ('wrong', 11),\n",
       " ('thinking', 11),\n",
       " ('ok', 11),\n",
       " ('dog', 11),\n",
       " ('nigga', 11),\n",
       " ('past', 11),\n",
       " ('annihilation', 11),\n",
       " ('film', 11),\n",
       " ('kill', 11),\n",
       " ('bed', 11),\n",
       " ('learn', 11),\n",
       " ('arsonist', 11),\n",
       " ('nice', 11),\n",
       " ('star', 11),\n",
       " ('bioterror', 11),\n",
       " ('mad', 11),\n",
       " ('enough', 11),\n",
       " ('pain', 11),\n",
       " ('running', 11),\n",
       " ('control', 11),\n",
       " ('gave', 11),\n",
       " ('broke', 11),\n",
       " ('land', 11),\n",
       " ('purse', 11),\n",
       " ('saying', 11),\n",
       " ('casualty', 11),\n",
       " ('watching', 11),\n",
       " ('alone', 11),\n",
       " ('service', 11),\n",
       " ('explosion', 11),\n",
       " ('chance', 11),\n",
       " ('angry', 11),\n",
       " ('desolate', 11),\n",
       " ('cat', 11),\n",
       " ('offroad', 11),\n",
       " ('lamp', 11),\n",
       " ('flooding', 11),\n",
       " ('gems', 11),\n",
       " ('landslide', 11),\n",
       " ('seismic', 11),\n",
       " ('snowstorm', 11),\n",
       " ('r', 10),\n",
       " ('means', 10),\n",
       " ('guess', 10),\n",
       " ('wants', 10),\n",
       " ('tried', 10),\n",
       " ('mode', 10),\n",
       " ('shall', 10),\n",
       " ('career', 10),\n",
       " ('park', 10),\n",
       " ('sign', 10),\n",
       " ('share', 10),\n",
       " ('jeb', 10),\n",
       " ('wake', 10),\n",
       " ('till', 10),\n",
       " ('direction', 10),\n",
       " ('arson', 10),\n",
       " ('business', 10),\n",
       " ('needs', 10),\n",
       " ('future', 10),\n",
       " ('weather', 10),\n",
       " ('haha', 10),\n",
       " ('dude', 10),\n",
       " ('follow', 10),\n",
       " ('glass', 10),\n",
       " ('lets', 10),\n",
       " ('entire', 10),\n",
       " ('says', 10),\n",
       " ('order', 10),\n",
       " ('mom', 10),\n",
       " ('though', 10),\n",
       " ('buy', 10),\n",
       " ('handbags', 10),\n",
       " ('child', 10),\n",
       " ('omg', 10),\n",
       " ('days', 10),\n",
       " ('state', 10),\n",
       " ('fires', 10),\n",
       " ('absolutely', 10),\n",
       " ('cable', 10),\n",
       " ('de', 10),\n",
       " ('else', 10),\n",
       " ('turned', 10),\n",
       " ('businesses', 10),\n",
       " ('apollo', 10),\n",
       " ('okay', 10),\n",
       " ('fatalities', 10),\n",
       " ('x', 10),\n",
       " ('mayhem', 10),\n",
       " ('rioting', 10),\n",
       " ('tragedy', 10),\n",
       " ('thanks', 9),\n",
       " ('taking', 9),\n",
       " ('st', 9),\n",
       " ('talk', 9),\n",
       " ('support', 9),\n",
       " ('aftershock', 9),\n",
       " ('behind', 9),\n",
       " ('second', 9),\n",
       " ('petition', 9),\n",
       " ('wild', 9),\n",
       " ('radio', 9),\n",
       " ('version', 9),\n",
       " ('block', 9),\n",
       " ('mod', 9),\n",
       " ('scared', 9),\n",
       " ('door', 9),\n",
       " ('history', 9),\n",
       " ('latest', 9),\n",
       " ('working', 9),\n",
       " ('drink', 9),\n",
       " ('media', 9),\n",
       " ('secrets', 9),\n",
       " ('favorite', 9),\n",
       " ('taken', 9),\n",
       " ('transport', 9),\n",
       " ('lab', 9),\n",
       " ('hair', 9),\n",
       " ('weekend', 9),\n",
       " ('ago', 9),\n",
       " ('party', 9),\n",
       " ('august', 9),\n",
       " ('giving', 9),\n",
       " ('bitch', 9),\n",
       " ('text', 9),\n",
       " ('wedding', 9),\n",
       " ('brought', 9),\n",
       " ('open', 9),\n",
       " ('playing', 9),\n",
       " ('dad', 9),\n",
       " ('super', 9),\n",
       " ('limited', 9),\n",
       " ('hobo', 9),\n",
       " ('children', 9),\n",
       " ('colour', 9),\n",
       " ('games', 9),\n",
       " ('wish', 9),\n",
       " ('road', 9),\n",
       " ('property', 9),\n",
       " ('front', 9),\n",
       " ('worlds', 9),\n",
       " ('driving', 9),\n",
       " ('centre', 9),\n",
       " ('pray', 9),\n",
       " ('shape', 9),\n",
       " ('become', 9),\n",
       " ('public', 9),\n",
       " ('smaug', 9),\n",
       " ('press', 9),\n",
       " ('devastated', 9),\n",
       " ('displaced', 9),\n",
       " ('planned', 9),\n",
       " ('evacuation', 9),\n",
       " ('famine', 9),\n",
       " ('fatal', 9),\n",
       " ('combo', 9),\n",
       " ('bus', 9),\n",
       " ('ancient', 9),\n",
       " ('aba', 9),\n",
       " ('delivers', 9),\n",
       " ('sinkhole', 9),\n",
       " ('crying', 8),\n",
       " ('building', 8),\n",
       " ('perfect', 8),\n",
       " ('michael', 8),\n",
       " ('teen', 8),\n",
       " ('sleeping', 8),\n",
       " ('double', 8),\n",
       " ('daily', 8),\n",
       " ('financial', 8),\n",
       " ('moment', 8),\n",
       " ('sometimes', 8),\n",
       " ('sent', 8),\n",
       " ('ship', 8),\n",
       " ('iran', 8),\n",
       " ('survivors', 8),\n",
       " ('least', 8),\n",
       " ('birthday', 8),\n",
       " ('kinda', 8),\n",
       " ('played', 8),\n",
       " ('lucky', 8),\n",
       " ('took', 8),\n",
       " ('asked', 8),\n",
       " ('give', 8),\n",
       " ('peace', 8),\n",
       " ('unless', 8),\n",
       " ('series', 8),\n",
       " ('loved', 8),\n",
       " ('earth', 8),\n",
       " ('united', 8),\n",
       " ('lead', 8),\n",
       " ('round', 8),\n",
       " ('sit', 8),\n",
       " ('green', 8),\n",
       " ('group', 8),\n",
       " ('worst', 8),\n",
       " ('feels', 8),\n",
       " ('boy', 8),\n",
       " ('team', 8),\n",
       " ('happened', 8),\n",
       " ('occurred', 8),\n",
       " ('involving', 8),\n",
       " ('fleets', 8),\n",
       " ('totaling', 8),\n",
       " ('ships', 8),\n",
       " ('department', 8),\n",
       " ('bioterrorism', 8),\n",
       " ('crazy', 8),\n",
       " ('lady', 8),\n",
       " ('traffic', 8),\n",
       " ('place', 8),\n",
       " ('beyond', 8),\n",
       " ('joe', 8),\n",
       " ('walking', 8),\n",
       " ('date', 8),\n",
       " ('w', 8),\n",
       " ('told', 8),\n",
       " ('couple', 8),\n",
       " ('rock', 8),\n",
       " ('walk', 8),\n",
       " ('mean', 8),\n",
       " ('standard', 8),\n",
       " ('somebody', 8),\n",
       " ('gotta', 8),\n",
       " ('rather', 8),\n",
       " ('post', 8),\n",
       " ('middle', 8),\n",
       " ('ross', 8),\n",
       " ('boat', 8),\n",
       " ('ppl', 8),\n",
       " ('apply', 8),\n",
       " ('catastrophic', 8),\n",
       " ('instead', 8),\n",
       " ('idea', 8),\n",
       " ('internet', 8),\n",
       " ('wave', 8),\n",
       " ('either', 8),\n",
       " ('room', 8),\n",
       " ('fans', 8),\n",
       " ('character', 8),\n",
       " ('lost', 8),\n",
       " ('hold', 8),\n",
       " ('deaths', 8),\n",
       " ('international', 8),\n",
       " ('wonder', 8),\n",
       " ('sensor', 8),\n",
       " ('devastation', 8),\n",
       " ('rain', 8),\n",
       " ('job', 8),\n",
       " ('access', 8),\n",
       " ('screen', 8),\n",
       " ('reasons', 8),\n",
       " ('travel', 8),\n",
       " ('mortal', 8),\n",
       " ('kombat', 8),\n",
       " ('forest', 8),\n",
       " ('desires', 8),\n",
       " ('rainstorm', 8),\n",
       " ('tornado', 8),\n",
       " ('chile', 8),\n",
       " ('lovely', 7),\n",
       " ('london', 7),\n",
       " ('sky', 7),\n",
       " ('alive', 7),\n",
       " ('risk', 7),\n",
       " ('issue', 7),\n",
       " ('drive', 7),\n",
       " ('icemoon', 7),\n",
       " ('murder', 7),\n",
       " ('able', 7),\n",
       " ('number', 7),\n",
       " ('episode', 7),\n",
       " ('country', 7),\n",
       " ('completely', 7),\n",
       " ('match', 7),\n",
       " ('started', 7),\n",
       " ('reading', 7),\n",
       " ('beat', 7),\n",
       " ('girls', 7),\n",
       " ('hand', 7),\n",
       " ('sense', 7),\n",
       " ('stage', 7),\n",
       " ('da', 7),\n",
       " ('navy', 7),\n",
       " ('owner', 7),\n",
       " ('gay', 7),\n",
       " ('town', 7),\n",
       " ('ice', 7),\n",
       " ('ready', 7),\n",
       " ('gop', 7),\n",
       " ('total', 7),\n",
       " ('bet', 7),\n",
       " ('worth', 7),\n",
       " ('original', 7),\n",
       " ('yo', 7),\n",
       " ('king', 7),\n",
       " ('climate', 7),\n",
       " ('special', 7),\n",
       " ('pool', 7),\n",
       " ('thousands', 7),\n",
       " ('facebook', 7),\n",
       " ('seek', 7),\n",
       " ('silver', 7),\n",
       " ('cold', 7),\n",
       " ('complete', 7),\n",
       " ('tears', 7),\n",
       " ('releases', 7),\n",
       " ('stories', 7),\n",
       " ('county', 7),\n",
       " ('train', 7),\n",
       " ('clutch', 7),\n",
       " ('cook', 7),\n",
       " ('floor', 7),\n",
       " ('seriously', 7),\n",
       " ('funny', 7),\n",
       " ('mary', 7),\n",
       " ('woke', 7),\n",
       " ('terrorist', 7),\n",
       " ('handbag', 7),\n",
       " ('vuitton', 7),\n",
       " ('air', 7),\n",
       " ('straight', 7),\n",
       " ('problem', 7),\n",
       " ('football', 7),\n",
       " ('metal', 7),\n",
       " ('together', 7),\n",
       " ('flat', 7),\n",
       " ('ahead', 7),\n",
       " ('others', 7),\n",
       " ('social', 7),\n",
       " ('sounds', 7),\n",
       " ('account', 7),\n",
       " ('prevent', 7),\n",
       " ('downtown', 7),\n",
       " ('center', 7),\n",
       " ('takes', 7),\n",
       " ('worse', 7),\n",
       " ('collision', 7),\n",
       " ('course', 7),\n",
       " ('guide', 7),\n",
       " ('shares', 7),\n",
       " ('disney', 7),\n",
       " ('info', 7),\n",
       " ('cyclone', 7),\n",
       " ('obama', 7),\n",
       " ('signed', 7),\n",
       " ('enugu', 7),\n",
       " ('trump', 7),\n",
       " ('grill', 7),\n",
       " ('different', 7),\n",
       " ('break', 7),\n",
       " ('america', 7),\n",
       " ('crackdown', 7),\n",
       " ('broad', 7),\n",
       " ('common', 7),\n",
       " ('security', 7),\n",
       " ('epicentre', 7),\n",
       " ('fantasy', 7),\n",
       " ('remove', 7),\n",
       " ('hostage', 7),\n",
       " ('hostages', 7),\n",
       " ('injured', 7),\n",
       " ('sandstorm', 7),\n",
       " ('threat', 7),\n",
       " ('built', 6),\n",
       " ('site', 6),\n",
       " ('gained', 6),\n",
       " ('followers', 6),\n",
       " ('quite', 6),\n",
       " ('single', 6),\n",
       " ('cruz', 6),\n",
       " ('due', 6),\n",
       " ('happen', 6),\n",
       " ('thomas', 6),\n",
       " ('youtube', 6),\n",
       " ('dream', 6),\n",
       " ('steve', 6),\n",
       " ('kick', 6),\n",
       " ('cannot', 6),\n",
       " ('dying', 6),\n",
       " ('vehicle', 6),\n",
       " ('choice', 6),\n",
       " ('hour', 6),\n",
       " ('easy', 6),\n",
       " ('fact', 6),\n",
       " ('short', 6),\n",
       " ('paul', 6),\n",
       " ('salt', 6),\n",
       " ('died', 6),\n",
       " ('dark', 6),\n",
       " ('based', 6),\n",
       " ('b', 6),\n",
       " ('toddler', 6),\n",
       " ('court', 6),\n",
       " ('feminists', 6),\n",
       " ('worry', 6),\n",
       " ('shift', 6),\n",
       " ('answer', 6),\n",
       " ('tweets', 6),\n",
       " ('lots', 6),\n",
       " ('fully', 6),\n",
       " ('possible', 6),\n",
       " ('ya', 6),\n",
       " ('australia', 6),\n",
       " ('germs', 6),\n",
       " ('usa', 6),\n",
       " ('word', 6),\n",
       " ('sun', 6),\n",
       " ('hands', 6),\n",
       " ('cute', 6),\n",
       " ('starts', 6),\n",
       " ('line', 6),\n",
       " ('wont', 6),\n",
       " ('max', 6),\n",
       " ('deep', 6),\n",
       " ('found', 6),\n",
       " ('art', 6),\n",
       " ('butter', 6),\n",
       " ('morning', 6),\n",
       " ('hurts', 6),\n",
       " ('stars', 6),\n",
       " ('pressure', 6),\n",
       " ('large', 6),\n",
       " ('feet', 6),\n",
       " ('listening', 6),\n",
       " ('meet', 6),\n",
       " ('supposed', 6),\n",
       " ('wow', 6),\n",
       " ('mini', 6),\n",
       " ('glad', 6),\n",
       " ('photos', 6),\n",
       " ('huge', 6),\n",
       " ('fashion', 6),\n",
       " ('monogram', 6),\n",
       " ('satchel', 6),\n",
       " ('tho', 6),\n",
       " ('saving', 6),\n",
       " ('write', 6),\n",
       " ('insurance', 6),\n",
       " ('shower', 6),\n",
       " ('ground', 6),\n",
       " ('minutes', 6),\n",
       " ('mind', 6),\n",
       " ('fat', 6),\n",
       " ('parents', 6),\n",
       " ('stone', 6),\n",
       " ('sex', 6),\n",
       " ('lose', 6),\n",
       " ('general', 6),\n",
       " ('opening', 6),\n",
       " ('pop', 6),\n",
       " ('lil', 6),\n",
       " ('hilarious', 6),\n",
       " ('oil', 6),\n",
       " ('german', 6),\n",
       " ('true', 6),\n",
       " ('ask', 6),\n",
       " ('gold', 6),\n",
       " ('dan', 6),\n",
       " ('san', 6),\n",
       " ('anti', 6),\n",
       " ('near', 6),\n",
       " ('stuff', 6),\n",
       " ('falling', 6),\n",
       " ('foxtrot', 6),\n",
       " ('di', 6),\n",
       " ('diving', 6),\n",
       " ('dangerous', 6),\n",
       " ('boys', 6),\n",
       " ('usually', 6),\n",
       " ('feed', 6),\n",
       " ('china', 6),\n",
       " ('conference', 6),\n",
       " ('replace', 6),\n",
       " ('kit', 6),\n",
       " ('mood', 6),\n",
       " ('derailed', 6),\n",
       " ('five', 6),\n",
       " ('stuart', 6),\n",
       " ('earlier', 6),\n",
       " ('killer', 6),\n",
       " ('detonation', 6),\n",
       " ('pussy', 6),\n",
       " ('information', 6),\n",
       " ('suicide', 6),\n",
       " ('caught', 6),\n",
       " ('sea', 6),\n",
       " ('hi', 6),\n",
       " ('quick', 6),\n",
       " ('interesting', 6),\n",
       " ('curved', 6),\n",
       " ('fog', 6),\n",
       " ('patience', 6),\n",
       " ('bayelsa', 6),\n",
       " ('camp', 6),\n",
       " ('test', 6),\n",
       " ('inundation', 6),\n",
       " ('murderer', 6),\n",
       " ('massacre', 6),\n",
       " ('syrian', 6),\n",
       " ('eat', 5),\n",
       " ('doubt', 5),\n",
       " ('mac', 5),\n",
       " ('route', 5),\n",
       " ('students', 5),\n",
       " ('cuz', 5),\n",
       " ('knew', 5),\n",
       " ('david', 5),\n",
       " ('jobs', 5),\n",
       " ('airplane', 5),\n",
       " ('automatic', 5),\n",
       " ('practice', 5),\n",
       " ('across', 5),\n",
       " ('ended', 5),\n",
       " ('stopped', 5),\n",
       " ('officially', 5),\n",
       " ('tonto', 5),\n",
       " ('false', 5),\n",
       " ('less', 5),\n",
       " ('voice', 5),\n",
       " ('books', 5),\n",
       " ('mountain', 5),\n",
       " ('imagine', 5),\n",
       " ('zombie', 5),\n",
       " ('fighting', 5),\n",
       " ('shot', 5),\n",
       " ('xbox', 5),\n",
       " ('mining', 5),\n",
       " ('rule', 5),\n",
       " ('movies', 5),\n",
       " ('prepare', 5),\n",
       " ('tracks', 5),\n",
       " ('fingers', 5),\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_normal = get_vocab(df_train[df_train['target'] == 0])\n",
    "V_normal.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 291),\n",
       " ('get', 211),\n",
       " ('via', 198),\n",
       " ('fire', 194),\n",
       " ('new', 192),\n",
       " ('people', 159),\n",
       " ('one', 155),\n",
       " ('emergency', 140),\n",
       " ('body', 117),\n",
       " ('would', 116),\n",
       " ('burning', 111),\n",
       " ('still', 107),\n",
       " ('suicide', 106),\n",
       " ('police', 101),\n",
       " ('got', 100),\n",
       " ('first', 99),\n",
       " ('video', 95),\n",
       " ('know', 94),\n",
       " ('going', 93),\n",
       " ('back', 92),\n",
       " ('disaster', 92),\n",
       " ('two', 91),\n",
       " ('see', 90),\n",
       " ('full', 89),\n",
       " ('buildings', 89),\n",
       " ('man', 88),\n",
       " ('bomb', 88),\n",
       " ('time', 87),\n",
       " ('us', 84),\n",
       " ('crash', 84),\n",
       " ('may', 83),\n",
       " ('love', 81),\n",
       " ('go', 80),\n",
       " ('last', 78),\n",
       " ('many', 78),\n",
       " ('nuclear', 78),\n",
       " ('killed', 77),\n",
       " ('day', 76),\n",
       " ('say', 75),\n",
       " ('think', 75),\n",
       " ('want', 74),\n",
       " ('news', 74),\n",
       " ('car', 73),\n",
       " ('train', 73),\n",
       " ('california', 72),\n",
       " ('could', 72),\n",
       " ('watch', 71),\n",
       " ('u', 71),\n",
       " ('attack', 70),\n",
       " ('years', 69),\n",
       " ('army', 69),\n",
       " ('world', 69),\n",
       " ('storm', 69),\n",
       " ('rt', 68),\n",
       " ('mass', 68),\n",
       " ('collapse', 67),\n",
       " ('dead', 66),\n",
       " ('make', 65),\n",
       " ('good', 65),\n",
       " ('bombing', 65),\n",
       " ('families', 65),\n",
       " ('really', 64),\n",
       " ('need', 64),\n",
       " ('accident', 63),\n",
       " ('another', 63),\n",
       " ('take', 63),\n",
       " ('fatal', 62),\n",
       " ('even', 61),\n",
       " ('way', 61),\n",
       " ('school', 58),\n",
       " ('look', 58),\n",
       " ('home', 58),\n",
       " ('work', 58),\n",
       " ('war', 58),\n",
       " ('atomic', 58),\n",
       " ('let', 58),\n",
       " ('bomber', 58),\n",
       " ('hiroshima', 57),\n",
       " ('life', 56),\n",
       " ('help', 56),\n",
       " ('never', 55),\n",
       " ('today', 55),\n",
       " ('every', 54),\n",
       " ('death', 54),\n",
       " ('city', 53),\n",
       " ('right', 53),\n",
       " ('fear', 53),\n",
       " ('forest', 52),\n",
       " ('fires', 52),\n",
       " ('read', 52),\n",
       " ('said', 51),\n",
       " ('old', 51),\n",
       " ('homes', 51),\n",
       " ('near', 50),\n",
       " ('please', 50),\n",
       " ('feel', 50),\n",
       " ('much', 49),\n",
       " ('oil', 49),\n",
       " ('getting', 48),\n",
       " ('im', 48)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = get_vocab(df_train)\n",
    "V.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'get',\n",
       " 'via',\n",
       " 'fire',\n",
       " 'new',\n",
       " 'people',\n",
       " 'one',\n",
       " 'emergency',\n",
       " 'body',\n",
       " 'would',\n",
       " 'burning',\n",
       " 'still',\n",
       " 'suicide',\n",
       " 'police',\n",
       " 'got',\n",
       " 'first',\n",
       " 'video',\n",
       " 'know',\n",
       " 'going',\n",
       " 'back',\n",
       " 'disaster',\n",
       " 'two',\n",
       " 'see',\n",
       " 'full',\n",
       " 'buildings',\n",
       " 'man',\n",
       " 'bomb',\n",
       " 'time',\n",
       " 'us',\n",
       " 'crash',\n",
       " 'may',\n",
       " 'love',\n",
       " 'go',\n",
       " 'last',\n",
       " 'many',\n",
       " 'nuclear',\n",
       " 'killed',\n",
       " 'day',\n",
       " 'say',\n",
       " 'think',\n",
       " 'want',\n",
       " 'news',\n",
       " 'car',\n",
       " 'train',\n",
       " 'california',\n",
       " 'could',\n",
       " 'watch',\n",
       " 'u',\n",
       " 'attack',\n",
       " 'years',\n",
       " 'army',\n",
       " 'world',\n",
       " 'storm',\n",
       " 'rt',\n",
       " 'mass',\n",
       " 'collapse',\n",
       " 'dead',\n",
       " 'make',\n",
       " 'good',\n",
       " 'bombing',\n",
       " 'families',\n",
       " 'really',\n",
       " 'need',\n",
       " 'accident',\n",
       " 'another',\n",
       " 'take',\n",
       " 'fatal',\n",
       " 'even',\n",
       " 'way',\n",
       " 'school',\n",
       " 'look',\n",
       " 'home',\n",
       " 'work',\n",
       " 'war',\n",
       " 'atomic',\n",
       " 'let',\n",
       " 'bomber',\n",
       " 'hiroshima',\n",
       " 'life',\n",
       " 'help',\n",
       " 'never',\n",
       " 'today',\n",
       " 'every',\n",
       " 'death',\n",
       " 'city',\n",
       " 'right',\n",
       " 'fear',\n",
       " 'forest',\n",
       " 'fires',\n",
       " 'read',\n",
       " 'said',\n",
       " 'old',\n",
       " 'homes',\n",
       " 'near',\n",
       " 'please',\n",
       " 'feel',\n",
       " 'much',\n",
       " 'oil',\n",
       " 'getting',\n",
       " 'im',\n",
       " 'debris',\n",
       " 'lol',\n",
       " 'top',\n",
       " 'truck',\n",
       " 'stop',\n",
       " 'cross',\n",
       " 'year',\n",
       " 'since',\n",
       " 'northern',\n",
       " 'evacuation',\n",
       " 'live',\n",
       " 'great',\n",
       " 'found',\n",
       " 'hope',\n",
       " 'injured',\n",
       " 'smoke',\n",
       " 'set',\n",
       " 'black',\n",
       " 'face',\n",
       " 'hit',\n",
       " 'without',\n",
       " 'natural',\n",
       " 'content',\n",
       " 'flood',\n",
       " 'shit',\n",
       " 'ever',\n",
       " 'little',\n",
       " 'come',\n",
       " 'severe',\n",
       " 'floods',\n",
       " 'coming',\n",
       " 'bloody',\n",
       " 'military',\n",
       " 'flames',\n",
       " 'heat',\n",
       " 'always',\n",
       " 'check',\n",
       " 'next',\n",
       " 'screaming',\n",
       " 'fall',\n",
       " 'water',\n",
       " 'spill',\n",
       " 'loud',\n",
       " 'flooding',\n",
       " 'gonna',\n",
       " 'god',\n",
       " 'released',\n",
       " 'attacked',\n",
       " 'trapped',\n",
       " 'damage',\n",
       " 'everyone',\n",
       " 'fucking',\n",
       " 'family',\n",
       " 'liked',\n",
       " 'movie',\n",
       " 'japan',\n",
       " 'sinking',\n",
       " 'thunderstorm',\n",
       " 'collided',\n",
       " 'lightning',\n",
       " 'night',\n",
       " 'air',\n",
       " 'services',\n",
       " 'high',\n",
       " 'charged',\n",
       " 'migrants',\n",
       " 'end',\n",
       " 'best',\n",
       " 'cause',\n",
       " 'ambulance',\n",
       " 'destroy',\n",
       " 'boy',\n",
       " 'bad',\n",
       " 'warning',\n",
       " 'wildfire',\n",
       " 'weather',\n",
       " 'thunder',\n",
       " 'reddit',\n",
       " 'outbreak',\n",
       " 'ruin',\n",
       " 'someone',\n",
       " 'state',\n",
       " 'looks',\n",
       " 'house',\n",
       " 'well',\n",
       " 'survive',\n",
       " 'says',\n",
       " 'bag',\n",
       " 'evacuate',\n",
       " 'deaths',\n",
       " 'dust',\n",
       " 'times',\n",
       " 'blood',\n",
       " 'whole',\n",
       " 'survivors',\n",
       " 'destruction',\n",
       " 'destroyed',\n",
       " 'real',\n",
       " 'keep',\n",
       " 'put']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = 200\n",
    "top_n_words = [w for w, c in V.most_common(n_words)]\n",
    "top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_top_n = CountVectorizer(vocabulary=top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1, 2)\n",
    "(2, 4)\n",
    "(3, 6)\n",
    "(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X=[[1], [2], [3], [4]], y=[2, 4, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.predict([[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.coef_\n",
    "y = coef_ * x + intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7763568394002505e-15"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = LinearRegression()\n",
    "\n",
    "m2.fit(X=[[2], [3], [4], [5]], y=[2, 4, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0000000000000036"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coef_* 2  + m2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.coef_* 6  + m2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.predict([[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vect_pipeline_v2(df_train, df_val, df_test, \n",
    "                        vectorizer, model,\n",
    "                        create_submission=False):\n",
    "    X_train = vectorizer.fit_transform(df_train['text'])\n",
    "    df_feature_train = pd.DataFrame(X_train.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    X_val = vectorizer.transform(df_val['text'])\n",
    "    df_feature_val = pd.DataFrame(X_val.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    X_test = vectorizer.transform(df_test['text'])\n",
    "    df_feature_test = pd.DataFrame(X_test.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    model.fit(df_feature_train, df_train['target'])\n",
    "    y_pred_train = model.predict(df_feature_train)\n",
    "    y_pred_val = model.predict(df_feature_val)\n",
    "    y_pred_test = model.predict(df_feature_test)\n",
    "    \n",
    "    \n",
    "    train_acc = accuracy_score(df_train['target'], y_pred_train)\n",
    "    val_acc = accuracy_score(df_val['target'], y_pred_val)\n",
    "    print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Val accuracy  : {val_acc:.3f}\")\n",
    "    \n",
    "    df_test['target'] = y_pred_test\n",
    "    if create_submission is not False:\n",
    "        df_test[['id', 'target']].to_csv(f\"{create_submission}.csv\", index=False)\n",
    "        \n",
    "    return model, df_feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.754\n",
      "Val accuracy  : 0.718\n"
     ]
    }
   ],
   "source": [
    "vect_top_n = CountVectorizer(vocabulary=top_n_words)\n",
    "m3 = LogisticRegression()\n",
    "m3, df_feature_train = count_vect_pipeline_v2(df_train, df_val, df_test, vect_top_n, m3, 'logistic_over_200_top_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>get</th>\n",
       "      <th>via</th>\n",
       "      <th>fire</th>\n",
       "      <th>new</th>\n",
       "      <th>people</th>\n",
       "      <th>one</th>\n",
       "      <th>emergency</th>\n",
       "      <th>body</th>\n",
       "      <th>would</th>\n",
       "      <th>...</th>\n",
       "      <th>dust</th>\n",
       "      <th>times</th>\n",
       "      <th>blood</th>\n",
       "      <th>whole</th>\n",
       "      <th>survivors</th>\n",
       "      <th>destruction</th>\n",
       "      <th>destroyed</th>\n",
       "      <th>real</th>\n",
       "      <th>keep</th>\n",
       "      <th>put</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   like  get  via  fire  new  people  one  emergency  body  would  ...  dust  \\\n",
       "0     0    0    0     0    0       0    0          0     0      0  ...     0   \n",
       "1     0    0    0     1    0       0    0          0     0      0  ...     0   \n",
       "2     0    0    0     0    0       0    0          0     0      0  ...     0   \n",
       "3     0    0    0     0    0       1    0          0     0      0  ...     0   \n",
       "4     0    0    0     0    0       0    0          0     0      0  ...     0   \n",
       "\n",
       "   times  blood  whole  survivors  destruction  destroyed  real  keep  put  \n",
       "0      0      0      0          0            0          0     0     0    0  \n",
       "1      0      0      0          0            0          0     0     0    0  \n",
       "2      0      0      0          0            0          0     0     0    0  \n",
       "3      0      0      0          0            0          0     0     0    0  \n",
       "4      0      0      0          0            0          0     0     0    0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = pd.DataFrame(zip(df_feature_train.columns, m3.coef_[0]), columns=['feature', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>full</td>\n",
       "      <td>-1.827851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>ruin</td>\n",
       "      <td>-1.645006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>love</td>\n",
       "      <td>-1.276482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>put</td>\n",
       "      <td>-1.135401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>fucking</td>\n",
       "      <td>-1.118411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>body</td>\n",
       "      <td>-1.033189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>god</td>\n",
       "      <td>-1.014095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>let</td>\n",
       "      <td>-0.983474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>lol</td>\n",
       "      <td>-0.964643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>check</td>\n",
       "      <td>-0.956705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature    weight\n",
       "23      full -1.827851\n",
       "179     ruin -1.645006\n",
       "31      love -1.276482\n",
       "199      put -1.135401\n",
       "151  fucking -1.118411\n",
       "8       body -1.033189\n",
       "145      god -1.014095\n",
       "75       let -0.983474\n",
       "101      lol -0.964643\n",
       "136    check -0.956705"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw.sort_values(\"weight\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>near</td>\n",
       "      <td>1.803395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>severe</td>\n",
       "      <td>1.850278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>killed</td>\n",
       "      <td>1.961608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>california</td>\n",
       "      <td>1.964473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>outbreak</td>\n",
       "      <td>2.074993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>spill</td>\n",
       "      <td>2.174942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>train</td>\n",
       "      <td>2.395358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>migrants</td>\n",
       "      <td>2.715518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>hiroshima</td>\n",
       "      <td>2.866165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>debris</td>\n",
       "      <td>2.890948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature    weight\n",
       "93         near  1.803395\n",
       "128      severe  1.850278\n",
       "36       killed  1.961608\n",
       "44   california  1.964473\n",
       "178    outbreak  2.074993\n",
       "141       spill  2.174942\n",
       "43        train  2.395358\n",
       "165    migrants  2.715518\n",
       "77    hiroshima  2.866165\n",
       "100      debris  2.890948"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw.sort_values(\"weight\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.846\n",
      "Val accuracy  : 0.769\n"
     ]
    }
   ],
   "source": [
    "n_words = 1000\n",
    "top_n_words = [w for w, c in V.most_common(n_words)]\n",
    "top_n_words\n",
    "vect_top_n = CountVectorizer(vocabulary=top_n_words)\n",
    "m1000 = LogisticRegression()\n",
    "m1000, df_feature_train = count_vect_pipeline_v2(df_train, df_val, df_test, vect_top_n, m1000, f'logistic_over_1000_top_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.921\n",
      "Val accuracy  : 0.776\n"
     ]
    }
   ],
   "source": [
    "n_words = 5000\n",
    "top_n_words = [w for w, c in V.most_common(n_words)]\n",
    "top_n_words\n",
    "vect_top_n = CountVectorizer(vocabulary=top_n_words)\n",
    "m = LogisticRegression()\n",
    "m, df_feature_train = count_vect_pipeline_v2(df_train, df_val, df_test, vect_top_n, m, f'logistic_over_{n_words}_top_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>drought</td>\n",
       "      <td>1.807997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>storm</td>\n",
       "      <td>1.901454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>massacre</td>\n",
       "      <td>1.936732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>floods</td>\n",
       "      <td>1.961371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>debris</td>\n",
       "      <td>1.987077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>hailstorm</td>\n",
       "      <td>2.010973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>derailment</td>\n",
       "      <td>2.057129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>bombing</td>\n",
       "      <td>2.099211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>2.431265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>hiroshima</td>\n",
       "      <td>2.832892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature    weight\n",
       "454     drought  1.807997\n",
       "52        storm  1.901454\n",
       "257    massacre  1.936732\n",
       "129      floods  1.961371\n",
       "100      debris  1.987077\n",
       "835   hailstorm  2.010973\n",
       "602  derailment  2.057129\n",
       "59      bombing  2.099211\n",
       "560  earthquake  2.431265\n",
       "77    hiroshima  2.832892"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw = pd.DataFrame(zip(df_feature_train.columns, m.coef_[0]), columns=['feature', 'weight'])\n",
    "cw.sort_values(\"weight\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>full</td>\n",
       "      <td>-1.371386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ebay</td>\n",
       "      <td>-1.358745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>ass</td>\n",
       "      <td>-1.354927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295</th>\n",
       "      <td>poll</td>\n",
       "      <td>-1.313449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>song</td>\n",
       "      <td>-1.271447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>bags</td>\n",
       "      <td>-1.244992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>eyes</td>\n",
       "      <td>-1.226552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>ruin</td>\n",
       "      <td>-1.221095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>better</td>\n",
       "      <td>-1.203329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>join</td>\n",
       "      <td>-1.162025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature    weight\n",
       "23      full -1.371386\n",
       "307     ebay -1.358745\n",
       "318      ass -1.354927\n",
       "4295    poll -1.313449\n",
       "475     song -1.271447\n",
       "291     bags -1.244992\n",
       "614     eyes -1.226552\n",
       "179     ruin -1.221095\n",
       "228   better -1.203329\n",
       "795     join -1.162025"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw.sort_values(\"weight\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.870\n",
      "Val accuracy  : 0.795\n"
     ]
    }
   ],
   "source": [
    "n_words = 5000\n",
    "top_n_words = [w for w, c in V.most_common(n_words)]\n",
    "top_n_words\n",
    "vect_top_n = TfidfVectorizer(vocabulary=top_n_words)\n",
    "m = LogisticRegression()\n",
    "m, df_feature_train = count_vect_pipeline_v2(df_train, df_val, df_test, vect_top_n, m, f'logistic_over_{n_words}_top_words_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P( disaster \\mid \\text{this tweet})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Bayes%27_theorem\n",
    "\n",
    "$P(A\\mid B)=\\frac {P(B\\mid A) \\cdot P(A)}{P(B)}$\n",
    "\n",
    "\n",
    "$P( disaster \\mid \\text{this tweet}) =\\frac {P(\\text{this tweet}\\mid disaster) \\cdot P(disaster)}{P(\\text{this tweet})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.574369\n",
       "1    0.425631\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P( disaster \\mid \\text{this tweet})  \\varpropto P(\\text{this tweet}\\mid disaster) \\cdot P(disaster)$\n",
    "\n",
    "$P( non disaster \\mid \\text{this tweet})  \\varpropto P(\\text{this tweet}\\mid non disaster) \\cdot P(non disaster)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.861\n",
      "Val accuracy  : 0.770\n"
     ]
    }
   ],
   "source": [
    "n_words = 5000\n",
    "top_n_words = [w for w, c in V.most_common(n_words)]\n",
    "top_n_words\n",
    "vect_top_n = CountVectorizer(vocabulary=top_n_words)\n",
    "m = MultinomialNB()\n",
    "m, df_feature_train = count_vect_pipeline_v2(df_train, df_val, df_test, vect_top_n, m, f'nb_{n_words}_top_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.574369\n",
       "1    0.425631\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.57436871, 0.42563129])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "display(df_train['target'].value_counts(normalize=True))\n",
    "\n",
    "np.exp(m.class_log_prior_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P( disaster \\mid \\text{this tweet})  \\varpropto P(\\text{this tweet}\\mid disaster) \\cdot P(disaster)$\n",
    "\n",
    "$P( non disaster \\mid \\text{this tweet})  \\varpropto P(\\text{this tweet}\\mid non disaster) \\cdot P(non disaster)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{this tweet}\\mid disaster) = P(\\text{tweet words }\\mid disaster)$ \n",
    "\n",
    "P(w|disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07015357777016296"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(m.coef_).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.376642461280849"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m.coef_.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.657068472021267"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m.coef_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = pd.DataFrame(zip(top_n_words, np.exp(m.coef_[0])), columns=['word', 'proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>thinks</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>applications</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>volga</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>enter</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>devalue</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>mcilroy</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>tastes</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>headset</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>bluedio</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>password</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word     proba\n",
       "2499        thinks  0.000031\n",
       "3154  applications  0.000031\n",
       "3151         volga  0.000031\n",
       "3150         enter  0.000031\n",
       "3148       devalue  0.000031\n",
       "3146       mcilroy  0.000031\n",
       "3143        tastes  0.000031\n",
       "3142       headset  0.000031\n",
       "3141       bluedio  0.000031\n",
       "3130      password  0.000031"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp.sort_values(\"proba\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>hiroshima</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>california</td>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>disaster</td>\n",
       "      <td>0.002866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>people</td>\n",
       "      <td>0.003084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>news</td>\n",
       "      <td>0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fire</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>http</td>\n",
       "      <td>0.066073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>co</td>\n",
       "      <td>0.070154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     proba\n",
       "77     hiroshima  0.002710\n",
       "44    california  0.002804\n",
       "20      disaster  0.002866\n",
       "5         people  0.003084\n",
       "12       suicide  0.003427\n",
       "2            via  0.003582\n",
       "41          news  0.003956\n",
       "3           fire  0.005171\n",
       "4801        http  0.066073\n",
       "2177          co  0.070154"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp.sort_values(\"proba\").tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
